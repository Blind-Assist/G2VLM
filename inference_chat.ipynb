{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9649fffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from g2vlm_utils import load_model_and_tokenizer, build_transform, process_conversation\n",
    "\n",
    "def process_conversation(model_path, image_path, user_question):\n",
    "    \"\"\"\n",
    "    Process a conversation with an image and question using a vision-language model.\n",
    "    \n",
    "    This function loads a pre-trained model, processes an image and user question,\n",
    "    and returns the model's response based on the 3D scene understanding.\n",
    "    \n",
    "    Args:\n",
    "        model_path (str): Path to the pre-trained model checkpoint.\n",
    "        image_path (str): Path to the input image file. If None, uses default example.\n",
    "        user_question (str): The user's question about the image. If None, uses default question.\n",
    "    \n",
    "    Returns:\n",
    "        str: The model's response to the user's question.\n",
    "    \n",
    "    Example:\n",
    "        >>> response = process_conversation(\n",
    "        ...     model_path='path/to/model',\n",
    "        ...     image_path='examples/25_0.jpg',\n",
    "        ...     user_question='What is the depth of the object?'\n",
    "        ... )\n",
    "        >>> print(response)\n",
    "    \"\"\"\n",
    "    enable_template = True\n",
    "\n",
    "    model, tokenizer, new_token_ids , vit_image_transform, dino_transform = load_model_and_tokenizer(model_path=model_path)\n",
    "    image_transform = build_transform(pixel=768)\n",
    "\n",
    "    total_params = sum(p.numel() for p in model.parameters()) / 1e9\n",
    "    print(f'[test] total_params: {total_params}B')\n",
    "\n",
    "    img_path = 'examples/25_0.jpg'\n",
    "    question = \"If the table (red point) is positioned at 2.6 meters, estimate the depth of the clothes (blue point).  Calculate or judge based on the 3D center points of these objects. The unit is meter. Submit your response as one numeric value only.\"\n",
    "\n",
    "    post_prompt = \"Please answer the question using a single word or phrase.\"\n",
    "    templated_question =  \"\\n\" + question + \"\\n\" + post_prompt\n",
    "\n",
    "    if user_question is not None: \n",
    "        templated_question = user_question\n",
    "    \n",
    "    if image_path is not None:\n",
    "        img_path = image_path\n",
    "\n",
    "    print(question)\n",
    "\n",
    "    images = [Image.open(img_path).convert('RGB') ]\n",
    "    images, conversation = process_conversation(images, templated_question)\n",
    "\n",
    "    response = model.chat_with_recon(\n",
    "        tokenizer, \n",
    "        new_token_ids,\n",
    "        image_transform,\n",
    "        dino_transform,\n",
    "        images=images,\n",
    "        prompt=conversation,\n",
    "        max_length=100,\n",
    "    )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfe96d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def process_videos(video_folder, model_path, output_excel):\n",
    "    \"\"\"\n",
    "    Load videos, sample frames, query the model, and save navigation instructions.\n",
    "    \n",
    "    Args:\n",
    "        video_folder (str): Path to folder containing videos.\n",
    "        model_path (str): Path to the pre-trained model checkpoint.\n",
    "        output_excel (str): Path to save the Excel file with instructions.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    # Prompt for navigation instructions\n",
    "    prompt = (\n",
    "        \"\"\"You are an assistive navigation system for a visually impaired user. Analyze the provided video from the user's forward perspective. \n",
    "        Identify all the immediate, high-risk obstructions. \n",
    "        State the obstruction's location using the 12-hour clock face. \n",
    "        Process the provided video and generate a single, actionable safety alert.\"\"\"\n",
    "    )\n",
    "\n",
    "    # Iterate over all videos in the folder\n",
    "    for video_file in os.listdir(video_folder):\n",
    "        if not video_file.lower().endswith((\".mp4\", \".avi\", \".mov\")):\n",
    "            continue\n",
    "\n",
    "        video_path = os.path.join(video_folder, video_file)\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "        frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        instructions = []\n",
    "\n",
    "        # Sample every 3rd frame\n",
    "        for i in range(0, frame_count, 3):\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                continue\n",
    "\n",
    "            # Save frame temporarily\n",
    "            frame_path = f\"temp_frame_{i}.jpg\"\n",
    "            cv2.imwrite(frame_path, frame)\n",
    "\n",
    "            # Query the model\n",
    "            response = process_conversation(\n",
    "                model_path=model_path,\n",
    "                image_path=frame_path,\n",
    "                user_question=prompt\n",
    "            )\n",
    "            instructions.append(response)\n",
    "\n",
    "            # Clean up temp frame\n",
    "            os.remove(frame_path)\n",
    "\n",
    "        cap.release()\n",
    "\n",
    "        # Store aggregated instructions for this video\n",
    "        results.append({\n",
    "            \"video\": video_file,\n",
    "            \"instructions\": \" \".join(instructions)  # combine responses\n",
    "        })\n",
    "\n",
    "    # Save all results to Excel\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_excel(output_excel, index=False)\n",
    "    print(f\"Saved navigation instructions to {output_excel}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2198203a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example usage\n",
    "video_folder = \"../../G2VLM/videos\"   # folder with your videos\n",
    "model_path = \"path/to/model\"          # replace with actual model checkpoint\n",
    "output_excel = \"navigation_instructions.xlsx\"\n",
    "process_videos(video_folder, model_path, output_excel)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
